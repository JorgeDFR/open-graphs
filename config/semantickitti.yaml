# Directory Paths
workspace_dir: /home/user/workspace
dataset_dir: /home/user/data/SemanticKitti
models_dir: /home/user/data/models
results_dir: ${workspace_dir}/results
opengraphs_dir: ${workspace_dir}/open-graphs

# Dataset
basedir: ${dataset_dir}/sequences/
sequence: !!str 05
stride: 10  # image interval
start: 0
end: 100

# Caption generation
save_vis: !!bool False  # whether to save visualization
save_cap: !!bool True   # whether to save caption data
save_vis_path: ${results_dir}/${sequence}/vis/
save_cap_path: ${results_dir}/${sequence}/caption/

# Models
tag2text_path: ${models_dir}/tag2text/tag2text_swin_14m.pth
gd_path: ${models_dir}/groundingdino/groundingdino_swint_ogc.py
gd_weights: ${models_dir}/groundingdino/groundingdino_swint_ogc.pth
tap_path: ${models_dir}/tokenize-anything/tap_vit_l_v1_0.pkl
tap_merge_path: ${models_dir}/tokenize-anything/merged_2560.pkl
llama_ckpt_dir: ${models_dir}/llama2/llama-2-7b-chat/
llama_tokenizer_path: ${models_dir}/llama2/llama-2-7b-chat/tokenizer.model
sbert_path: ${models_dir}/sbert/all-minilm-l6-v2
mos_path: ${models_dir}/4dmos/10_scans.ckpt

# Point cloud map generation
filter_dis: !!bool False                             # whether to filter objects that are too far away
max_depth: 100                                       # maximum point cloud distance; don’t set too large
bg_rate: 0.80                                        # probability threshold for determining background
min_points_threshold: 80                             # drop any instance with fewer points than this in a single frame
filter_dynamic: !!bool True                          # whether to filter dynamic objects
moving_thre: 0.5                                     # threshold to determine dynamic objects
dbscan_remove_noise: !!bool True                     # whether to remove noise using clustering
dbscan_eps: 0.5                                      # clustering parameter
dbscan_min_points: 20                                # clustering parameter
voxel_size: 0.1                                      # point cloud downsampling value
vis_all: !!bool False                                # whether to visualize the map before final processing

# Incremental fusion
use_bg: !!bool True                                  # whether to use background (treat background as an object)
spatial_weight: 1.0                                  # weight for spatial similarity — very important
caption_weight: 0.2                                  # weight for caption similarity
ft_weight: 0.4                                       # weight for feature similarity
sim_threshold: 0.6                                   # threshold for combined similarity score
llama_max_seq_len: 512                               # maximum input token length for llama
llama_max_batch_size: 8                              # maximum number of dialogs processed by llama at once
caption_merge_ft: !!bool True                        # whether to compute features after caption fusion

# Map saving and post-processing
save_pcd: !!bool True                                # whether to save the final map
save_pcd_path: ${results_dir}/${sequence}/pcd/       # location to save the pcd map
obj_min_points: 50                                   # minimum points required for an object in the final map
obj_min_detections: 1                                # minimum number of observations required for an object; many appear only once
merge_final: !!bool True                             # whether to merge in the final map
merge_overlap_thresh: 0.1                            # threshold for excessive overlap that requires merging
merge_ft_thresh: 0.3                                 # feature similarity threshold for final merging
max_caption_num: 4                                   # maximum number of accumulated captions; too many cause llama failures
class_methods: llama                                 # method to determine final class names: sbert / llama / gpt
gpt_max_num: 30                                      # maximum number of items per GPT query; too many cause errors later
spacy: !!bool True                                   # when class_methods=sbert, whether to use spaCy for extraction first
caption_only: !!bool False                           # when spacy=True, whether to use pure captions only for class alignment
openai_key: your_key                                 # key for GPT-4
api_base: your_api                                   # OpenAI proxy endpoint for China
class_colors_json: ${opengraphs_dir}/class_colors/class_colors.json  # semanticKITTI class + color JSON file

# Visualization
vis_sequence: !!str 05
result_path: ${results_dir}/${vis_sequence}/pcd/full_pcd.pkl.gz            # path to read object results
vis_ft_weight: 0.5                                                         # feature weight for multimodal retrieval
vis_caption_weight: 0.5                                                    # caption weight for multimodal retrieval
scenegraph_path: ${results_dir}/${vis_sequence}/pcd/object_relations.json  # object relationship file path
scenegraph_vis: !!bool False                                               # whether to visualize scene graph
save_image_vis_path: ${results_dir}/image_query/vis/                       # directory for image retrieval visualization
save_image_vis: !!bool False                                               # whether to save image retrieval results
no_sbert: !!bool False                                                     # do not use sbert for retrieval
no_tap: !!bool False                                                       # do not use tap model for caption extraction

# Scene graph
save_lane_path: ${results_dir}/${sequence}/graph+road.json                 # save path for road network
our_pcd: ${results_dir}/${sequence}/pcd/our_pc.pcd                         # our semantic map
